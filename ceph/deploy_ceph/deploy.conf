[global]
    filestore_fd_cache_size = 204800
    filestore_omap_header_cache_size = 204800

    osd pool default pg num = 32
    osd pool default pgp num = 32
    osd pool default size = 2
    osd pool default min size = 1

    public network = 10.1.0.0/16
    cluster network = 172.16.1.0/24

    ## allow ourselves to open a lot of files
    max open files = 320000
    mon osd min down reporters = 13
    mon_osd_min_down_reports = 8
    osd_heartbeat_interval = 10
    osd_heartbeat_grace = 60
    mon_osd_report_timeout = 1800
    log_to_syslog = false        ; uncomment this line to log to syslog
    ms_nocrc = true
    ms_bind_port_min = 6800
    ms_bind_port_max = 7000

    filestore_fiemap = true

##################
## Monitors
## You need at least one. You need at least three if you want to
## tolerate any node failures. Always create an odd number.
[mon]
    hosts = ceph21, ceph22, ceph23

    mon clock drift allowed    = .15
    mon osd down out interval = 1800
    mon_pg_warn_max_object_skew = 0


[osd]
    hosts = ceph21, ceph22, ceph23

    osd.ceph21.devs = /dev/vdb, /dev/vdc
    osd.ceph22.devs = /dev/vdb, /dev/vdc
    osd.ceph23.devs = /dev/vdb, /dev/vdc

    # osd journal = /data/cache/osd$id/journal
    # osd journal size = 10000	;journal size, in megabytes

    osd_recovery_threads = 1
    osd recovery max active = 2
    osd_max_backfills = 2
    osd_recovery_max_chunk = 4194304
    osd_recovery_op_priority = 5

    osd mkfs type = xfs
    osd mount options xfs = rw,noatime,inode64,logbsize=256k
    osd mkfs options xfs = -f -i size=2048

    osd op threads = 4
    osd disk threads = 2
    osd_map_cache_size = 1024
    osd_op_num_threads_per_shard = 2
    osd_op_num_shards = 15

    osd_op_thread_timeout = 5
    osd op history size = 20

    filestore queue max ops = 500
    filestore queue max bytes = 1048576000
    filestore queue committing max bytes = 1048576000
    filestore queue committing max ops = 5000
    filestore op threads = 8
    filestore journal writeahead = true

    ## filesytem config
    filestore_wbthrottle_xfs_ios_start_flusher = 10000000
    filestore_wbthrottle_xfs_ios_hard_limit = 10000000
    filestore_wbthrottle_xfs_inodes_start_flusher = 10000
    filestore_wbthrottle_xfs_inodes_hard_limit = 10000
    filestore_wbthrottle_xfs_bytes_start_flusher = 419430400
    filestore_wbthrottle_xfs_bytes_hard_limit = 4194304000

    journal max write entries = 5000
    journal queue max ops = 5000
    objecter inflight ops = 819200
    journal max write bytes = 10485760000
    journal queue max bytes = 10485760000
    ms dispatch throttle bytes = 1048576000
    objecter inflight op bytes = 1048576000
    osd_client_message_cap = 1000000
    osd_client_message_size_cap = 2147483648
    osd_max_write_size = 512

    ## manage the CRUSH map entirely manually
    ## only need configure this after we had set our crushmap
    osd crush update on start = true
    mutex_perf_counter = true

[client]
    rbd cache = false
    rbd cache size = 67108864
    rbd cache max dirty = 33554432
    rbd cache max dirty age = 5
    rbd cache writethrough until flush = true
    rbd_default_order = 25
    rbd_default_stripe_unit = 4194304
    rbd_default_stripe_count = 8
    admin socket = /var/run/ceph/ceph-client/$cluster-$type.$id.$pid.asok

[client.images]
    keyring = /etc/ceph/images.keyring
[client.volumes]
    keyring = /etc/ceph/volumes.keyring
[client.cinder]
    keyring = /etc/ceph/cinder.keyring
