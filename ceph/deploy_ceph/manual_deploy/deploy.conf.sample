[global]
    fsid = <UUID>
    mon_initial_members = <HOST_NAME>
    mon_host = <MONITOR_IP>
    auth_cluster_required = none
    auth_service_required = none
    auth_client_required = none

    filestore_fd_cache_size = 204800
    filestore_omap_header_cache_size = 204800

    osd_pool_default_pg_num = 32
    osd_pool_default_pgp_num = 32
    osd_pool_default_size = 2
    osd_pool_default_min_size = 1

    public_network = <MONITOR_IP>/24
    cluster_network = <MONITOR_IP>/24

    # allow ourselves to open a lot of files
    max_open_files = 320000
    mon_osd_min_down_reporters = 13
    mon_osd_min_down_reports = 8
    osd_heartbeat_interval = 10
    osd_heartbeat_grace = 60
    mon_osd_report_timeout = 1800
    # uncomment this line to log to syslog
    log_to_syslog = false
    ms_nocrc = true
    ms_bind_port_min = 6800
    ms_bind_port_max = 7000

    filestore_fiemap = false

##################
## Monitors
## You need at least one. You need at least three if you want to
## tolerate any node failures. Always create an odd number.
[mon]
    hosts = <HOST_NAME>

    mon_clock_drift_allowed = .15
    mon_osd_down_out_interval = 1800
    mon_pg_warn_max_object_skew = 0

[osd]
    hosts = <HOST_NAME>

    osd.<HOST_NAME>.devs = /dev/vdc, /dev/vdd, /dev/vde

    osd_recovery_threads = 1
    osd_recovery_max_active = 2
    osd_max_backfills = 2
    osd_recovery_max_chunk = 4194304
    osd_recovery_op_priority = 5

    osd_mkfs_type = xfs
    osd_mount_options_xfs = rw,nodev,noexec,noatime,nodiratime,attr2,discard,nobarrier,inode64,logbsize=256k,noquota
    osd_mkfs_options_xfs = -f -i size=2048

    osd_op_threads = 4
    osd_disk_threads = 2
    osd_map_cache_size = 1024
    osd_op_num_threads_per_shard = 2
    osd_op_num_shards = 15

    osd_op_thread_timeout = 5
    osd_op_history_size = 20

    filestore_queue_max_ops = 500
    filestore_queue_max_bytes = 1048576000
    filestore_queue_committing_max_bytes = 1048576000
    filestore_queue_committing_max_ops = 5000
    filestore_op_threads = 8
    filestore_journal_writeahead = true

    # filesytem config
    filestore_wbthrottle_xfs_ios_start_flusher = 10000000
    filestore_wbthrottle_xfs_ios_hard_limit = 10000000
    filestore_wbthrottle_xfs_inodes_start_flusher = 10000
    filestore_wbthrottle_xfs_inodes_hard_limit = 10000
    filestore_wbthrottle_xfs_bytes_start_flusher = 419430400
    filestore_wbthrottle_xfs_bytes_hard_limit = 4194304000

    journal_max_write_entries = 5000
    journal_queue_max_ops = 5000
    objecter_inflight_ops = 819200
    journal_max_write_bytes = 10485760000
    journal_queue_max_bytes = 10485760000
    ms_dispatch_throttle_bytes = 1048576000
    objecter_inflight_op_bytes = 1048576000
    osd_client_message_cap = 1000000
    osd_client_message_size_cap = 2147483648
    osd_max_write_size = 512

    # manage the CRUSH map automatically, configure this after we had set our crushmap
    osd_crush_update_on_start = true
    mutex_perf_counter = true

[client]
    rbd_cache = false
    rbd_cache_size = 67108864
    rbd_cache_max_dirty = 33554432
    rbd_cache_max_dirty_age = 5
    rbd_cache_writethrough_until_flush = true
    rbd_default_stripe_unit = 65536
    rbd_default_stripe_count = 16

[client.images]
    keyring = /etc/ceph/images.keyring
[client.volumes]
    keyring = /etc/ceph/volumes.keyring
[client.cinder]
    keyring = /etc/ceph/cinder.keyring

[client.rgw.<HOST_NAME>]
    rgw_frontends = "civetweb port=7480"
    host = <HOST_NAME>
    rgw_data = /var/lib/ceph/radosgw/ceph-$id/
    log_file = /var/log/ceph/ceph-client.$id.log
    user = synchronization-user
    rgw_zone = debug
    rgw_cache_enabled = true
    rgw_cache_lru_size = 10000
    rgw_print_continue = true
    rgw_thread_pool_size = 16
    rgw_override_bucket_index_max_shards = 20
    rgw_obj_stripe_size = 4194304
    debug_rgw = 20
    #debug_civetweb = 10
    rgw_enable_log_rados = true
    rgw_enable_ops_log = true
    #rgw_enable_apis = s3
